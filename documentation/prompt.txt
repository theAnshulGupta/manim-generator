\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\title{Gradient Descent}
\author{Formatted from MIT 6.390 Intro to Machine Learning notes}
\date{\today}

% theorem environment
\newtheorem{theorem}{Theorem}

\begin{document}

\maketitle
\tableofcontents
\bigskip

\section{Gradient Descent}

In supervised learning we often want to find the parameter set
\(
\Theta^\star=\arg\min_{\Theta} J(\Theta)
\)
for some objective \(J\).
When \(J\) is too complicated for closed‑form optimisation—or the parameter
dimension is simply huge—we turn to iterative optimisation.
The simplest workhorse is \emph{gradient descent} (GD).%
\footnote{A concise optimisation reference that underlies much of modern ML.}

Intuitively, imagine \(J(\Theta)\) as a terrain.
GD starts from an initial point, looks for the steepest downward direction, steps a
little, then repeats until it reaches (a local) minimum.

\vspace{.5em}
The chapter develops GD in four stages:

\begin{enumerate}[leftmargin=*]
  \item 1‑D gradient descent;
  \item extension to many dimensions;
  \item application to linear and ridge regression;
  \item stochastic gradient descent (SGD) for large data.
\end{enumerate}

% ------------------------------------------------------------
\subsection{Gradient descent in one dimension}\label{sec:1d}

Assume \(\Theta\in\mathbb{R}\) and both the objective
\(f(\Theta)\) and its derivative \(f'(\Theta)\) are available.
Besides those, GD needs three hyper‑parameters:

\begin{itemize}
  \item initial value \(\Theta_{\text{init}}\);
  \item step size (``learning rate'') \(\eta>0\);
  \item accuracy threshold \(\epsilon>0\).
\end{itemize}

Even with constant \(\eta\), the update magnitude changes because it is scaled
by the current derivative.

\begin{algorithm}[h]
  \caption{1D‑Gradient‑Descent}
  \begin{algorithmic}[1]
    \Procedure{1D‑Gradient‑Descent}{$\Theta_{\text{init}},\eta,f,f',\epsilon$}
      \State $\Theta^{(0)}\gets\Theta_{\text{init}}$, \quad $t\gets0$
      \Repeat
        \State $t\gets t+1$
        \State $\Theta^{(t)}\gets\Theta^{(t-1)}-\eta\,f'(\Theta^{(t-1)})$
      \Until{$\lvert f'(\Theta^{(t)})\rvert<\epsilon$}
      \State \Return $\Theta^{(t)}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Common alternative stopping rules:

\begin{enumerate}[label=(\alph*)]
  \item fixed iteration cap \(t=T\);
  \item small parameter change
        \(\lvert\Theta^{(t)}-\Theta^{(t-1)}\rvert<\epsilon\);
  \item small function change
        \(\lvert f(\Theta^{(t)})-f(\Theta^{(t-1)})\rvert<\epsilon\).
\end{enumerate}

\begin{theorem}
  \label{thm:1d-convex}
  Fix any \(\tilde\epsilon>0\).  
  If \(f\) is convex, differentiable, has a global minimum, and \(\eta\) is small
  enough, then GD reaches a point within \(\tilde\epsilon\) of an
  optimum.
\end{theorem}

\noindent
\emph{Example.}  
For \(f(x)=(x-2)^2\) starting at \(x_{\text{init}}=4\) with \(\eta=\tfrac12\),
GD converges quadratically.

% Optional figure; uncomment when you have the file
% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=.8\linewidth]{gd_convex.png}
%   \caption{Convergence on a convex quadratic.}
% \end{figure}

If \(f\) is non‑convex, convergence depends on the start point and step size.
GD can also stall at non‑minimiser stationary points (e.g.\ \(f(x)=x^{3}\))
or diverge when no minimum exists (e.g.\ \(f(x)=e^{-x}\)).

% ------------------------------------------------------------
\subsection{Multiple dimensions}

Let \(\Theta\in\mathbb{R}^m\) and \(f:\mathbb{R}^m\to\mathbb{R}\).
The gradient is
\[
\nabla_\Theta f(\Theta)=
\begin{bmatrix}
  \partial f/\partial\Theta_1\\
  \vdots\\
  \partial f/\partial\Theta_m
\end{bmatrix}.
\]
The update becomes
\(
\Theta^{(t)}
  =\Theta^{(t-1)}-\eta\nabla_\Theta f(\Theta^{(t-1)}).
\)
Termination based on function change
\(\lvert f(\Theta^{(t)})-f(\Theta^{(t-1)})\rvert<\epsilon\)
works in any dimension.

% ------------------------------------------------------------
\subsection{Application to regression}

Linear regression with squared‑error loss:
\[
J(\bm\theta)=\frac1n\sum_{i=1}^n
  \bigl(\bm\theta^{\!\top}\!x^{(i)}-y^{(i)}\bigr)^2,\qquad
\nabla_{\bm\theta}J=\frac{2}{n}X^\top(X\bm\theta-Y).
\]
GD update:
\[
\bm\theta^{(t)}
  =\bm\theta^{(t-1)}
    -\eta\frac{2}{n}\sum_{i=1}^{n}
        \bigl(\bm\theta^{(t-1)\!\top}x^{(i)}-y^{(i)}\bigr)x^{(i)}.
\]

\subsubsection{Ridge regression}

Add \(\ell_2\) regularisation:
\[
J_{\text{ridge}}(\bm\theta,\theta_0)
  =\frac1n\sum_{i=1}^n
     \bigl(\bm\theta^{\!\top}\!x^{(i)}+\theta_0-y^{(i)}\bigr)^2
   +\lambda\lVert\bm\theta\rVert^2.
\]
Gradients
\begin{align*}
\nabla_{\bm\theta}J_{\text{ridge}}
   &=\frac{2}{n}\sum_{i=1}^n
       \bigl(\bm\theta^{\!\top}\!x^{(i)}+\theta_0-y^{(i)}\bigr)x^{(i)}
       +2\lambda\bm\theta, \\
\partial J_{\text{ridge}}/\partial\theta_0
   &=\frac{2}{n}\sum_{i=1}^n
       \bigl(\bm\theta^{\!\top}\!x^{(i)}+\theta_0-y^{(i)}\bigr).
\end{align*}

\begin{algorithm}[h]
  \caption{RR‑Gradient‑Descent (ridge regression)}
  \begin{algorithmic}[1]
    \Procedure{RR‑Gradient‑Descent}{$\bm\theta_{\text{init}},
            \theta_{0,\text{init}},\eta,\epsilon$}
      \State $\bm\theta^{(0)}\gets\bm\theta_{\text{init}}$,
             $\theta_0^{(0)}\gets\theta_{0,\text{init}}$, $t\gets0$
      \Repeat
        \State $t\gets t+1$
        \State $\bm\theta^{(t)}\gets\bm\theta^{(t-1)}
          -\eta\Bigl[
            \tfrac1n\sum_{i}
              (\bm\theta^{(t-1)\!\top}x^{(i)}+\theta_0^{(t-1)}-y^{(i)})x^{(i)}
            +\lambda\bm\theta^{(t-1)}\Bigr]$
        \State $\theta_0^{(t)}\gets\theta_0^{(t-1)}
          -\eta\,\tfrac1n\sum_{i}
              (\bm\theta^{(t-1)\!\top}x^{(i)}+\theta_0^{(t-1)}-y^{(i)})$
      \Until{$\lvert
          J_{\text{ridge}}(\bm\theta^{(t)},\theta_0^{(t)})
         -J_{\text{ridge}}(\bm\theta^{(t-1)},\theta_0^{(t-1)})
         \rvert<\epsilon$}
      \State \Return $(\bm\theta^{(t)},\theta_0^{(t)})$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

% ------------------------------------------------------------
\subsection{Stochastic gradient descent (SGD)}

When \(
f(\Theta)=\tfrac1n\sum_{i=1}^n f_i(\Theta)
\),
computing \(\nabla f\) exactly may be costly.
SGD updates with one randomly chosen term:

\begin{algorithm}[h]
  \caption{Stochastic‑Gradient‑Descent}
  \begin{algorithmic}[1]
    \Procedure{SGD}{$\Theta_{\text{init}},\eta,f,\{\nabla f_i\},T$}
      \State $\Theta^{(0)}\gets\Theta_{\text{init}}$
      \For{$t=1$ \textbf{to} $T$}
        \State choose \(i\sim\text{Uniform}\{1,\dots,n\}\)
        \State $\Theta^{(t)}\gets\Theta^{(t-1)}
          -\eta(t)\,\nabla f_i(\Theta^{(t-1)})$
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
  \label{thm:sgd}
  If \(f\) is convex and the learning‑rate sequence satisfies
  \(
    \sum_{t=1}^{\infty}\eta(t)=\infty,\;
    \sum_{t=1}^{\infty}\eta(t)^2<\infty,
  \)
  then SGD converges to the optimum with probability 1.
\end{theorem}

\paragraph{Why SGD?}
\begin{itemize}[leftmargin=*]
  \item \textbf{Efficiency.}  
        Large data sets: each step touches one (or a small batch of)
        data point(s), saving time and memory.
  \item \textbf{Escaping bad minima.}  
        Noise in the update may jump out of shallow local minima
        that trap batch GD.
  \item \textbf{Generalisation.}  
        Slight under‑optimisation can reduce over‑fitting, often leading
        to lower test error.
\end{itemize}

% Optional figure illustrating two SGD trajectories
% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=.8\linewidth]{sgd_paths.png}
%   \caption{Two different initial points converge to different minima.}
% \end{figure}

\end{document}

Be creative with the visualizations and keep it short (maybe around 1-2 minutes)!
