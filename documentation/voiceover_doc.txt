'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPI Reference - Manim Voiceover v0.3.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContents\n\n\n\n\n\nMenu\n\n\n\n\n\n\n\nExpand\n\n\n\n\n\nLight mode\n\n\n\n\n\n\n\n\n\n\n\n\n\nDark mode\n\n\n\n\n\n\nAuto light/dark mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHide navigation sidebar\n\n\nHide table of contents sidebar\n\n\n\n\n\nToggle site navigation sidebar\n\n\n\n\nManim Voiceover v0.3.7\n\n\n\nStar\n\n\n\nToggle Light / Dark / Auto color theme\n\n\n\n\n\n\nToggle table of contents sidebar\n\n\n\n\n\n\n\n\n\n\n\nManim Voiceover v0.3.7\n\n\n\n\n\n\n\nInstallation\nQuickstart\nSpeech Services\nExample Gallery\nTranslating Scenes\nAPI Reference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBack to top\n\n\n\n\nStar on Github\n\n\n\n\n\n\n\n\nEdit this page\n\n\n\nToggle Light / Dark / Auto color theme\n\n\n\n\n\n\nToggle table of contents sidebar\n\n\n\n\n\nAPI Reference#\nThis reference manual details modules, functions, and variables included in\nManim Voiceover, describing what they are and what they do. For learning how to use\nManim Voiceover, see Quickstart.\n\nVoiceover scene#\n\n\nclass VoiceoverScene(renderer=None, camera_class=<class \'manim.camera.camera.Camera\'>, always_update_mobjects=False, random_seed=None, skip_animations=False)[source]#\nBases: Scene\nA scene class that can be used to add voiceover to a scene.\n\n\nadd_voiceover_text(text, subcaption=None, max_subcaption_len=70, subcaption_buff=0.1, **kwargs)[source]#\nAdds voiceover to the scene.\n\nParameters:\n\ntext (str) – The text to be spoken.\nsubcaption (Optional[str], optional) – Alternative subcaption text. If not specified, text is chosen as the subcaption. Defaults to None.\nmax_subcaption_len (int, optional) – Maximum number of characters for a subcaption. Subcaptions that are longer are split into chunks that are smaller than max_subcaption_len. Defaults to 70.\nsubcaption_buff (float, optional) – The duration between split subcaption chunks in seconds. Defaults to 0.1.\n\n\nReturns:\nThe tracker object for the voiceover.\n\nReturn type:\nVoiceoverTracker\n\n\n\n\n\nadd_wrapped_subcaption(subcaption, duration, subcaption_buff=0.1, max_subcaption_len=70)[source]#\nAdds a subcaption to the scene. If the subcaption is longer than max_subcaption_len, it is split into chunks that are smaller than max_subcaption_len.\n\nParameters:\n\nsubcaption (str) – The subcaption text.\nduration (float) – The duration of the subcaption in seconds.\nmax_subcaption_len (int, optional) – Maximum number of characters for a subcaption. Subcaptions that are longer are split into chunks that are smaller than max_subcaption_len. Defaults to 70.\nsubcaption_buff (float, optional) – The duration between split subcaption chunks in seconds. Defaults to 0.1.\n\n\nReturn type:\nNone\n\n\n\n\n\nsafe_wait(duration)[source]#\nWaits for a given duration. If the duration is less than one frame, it waits for one frame.\n\nParameters:\nduration (float) – The duration to wait for in seconds.\n\nReturn type:\nNone\n\n\n\n\n\nset_speech_service(speech_service, create_subcaption=True)[source]#\nSets the speech service to be used for the voiceover. This method\nshould be called before adding any voiceover to the scene.\n\nParameters:\n\nspeech_service (SpeechService) – The speech service to be used.\ncreate_subcaption (bool, optional) – Whether to create subcaptions for the scene. Defaults to True. If config.save_last_frame is True, the argument is\ncreated. (ignored and no subcaptions will be) – \n\n\nReturn type:\nNone\n\n\n\n\n\nvoiceover(text=None, ssml=None, **kwargs)[source]#\nThe main function to be used for adding voiceover to a scene.\n\nParameters:\n\ntext (str, optional) – The text to be spoken. Defaults to None.\nssml (str, optional) – The SSML to be spoken. Defaults to None.\n\n\nYields:\nGenerator[VoiceoverTracker, None, None] – The voiceover tracker object.\n\nReturn type:\nGenerator[VoiceoverTracker, None, None]\n\n\n\n\n\nwait_for_voiceover()[source]#\nWaits for the voiceover to finish.\n\nReturn type:\nNone\n\n\n\n\n\nwait_until_bookmark(mark)[source]#\nWaits until a bookmark is reached.\n\nParameters:\nmark (str) – The mark attribute of the bookmark to wait for.\n\nReturn type:\nNone\n\n\n\n\n\n\nclass VoiceoverTracker(scene, data, cache_dir)[source]#\nBases: object\nClass to track the progress of a voiceover in a scene.\nInitializes a VoiceoverTracker object.\n\nParameters:\n\nscene (Scene) – The scene to which the voiceover belongs.\npath (str) – The path to the JSON file containing the voiceover data.\ndata (dict) – \ncache_dir (str) – \n\n\n\n\n\nget_remaining_duration(buff=0.0)[source]#\nReturns the remaining duration of the voiceover.\n\nParameters:\nbuff (float, optional) – A buffer to add to the remaining duration. Defaults to 0.\n\nReturns:\nThe remaining duration of the voiceover in seconds.\n\nReturn type:\nint\n\n\n\n\n\ntime_until_bookmark(mark, buff=0, limit=None)[source]#\nReturns the time until a bookmark.\n\nParameters:\n\nmark (str) – The mark attribute of the bookmark to count up to.\nbuff (int, optional) – A buffer to add to the remaining duration, in seconds. Defaults to 0.\nlimit (Optional[int], optional) – A maximum value to return. Defaults to None.\n\n\nReturn type:\nint\n\n\n\n\n\n\nSpeech services#\n\n\nclass SpeechService(global_speed=1.0, cache_dir=None, transcription_model=None, transcription_kwargs={}, **kwargs)[source]#\nBases: ABC\nAbstract base class for a speech service.\n\nParameters:\n\nglobal_speed (float, optional) – The speed at which to play the audio.\nDefaults to 1.00.\ncache_dir (str, optional) – The directory to save the audio\nfiles to. Defaults to voiceovers/.\ntranscription_model (str, optional) – The\nOpenAI Whisper model\nto use for transcription. Defaults to None.\ntranscription_kwargs (dict, optional) – Keyword arguments to\npass to the transcribe() function. Defaults to {}.\n\n\n\n\n\naudio_callback(audio_path, data, **kwargs)[source]#\nCallback function for when the audio file is ready.\nOverride this method to do something with the audio file, e.g. noise reduction.\n\nParameters:\n\naudio_path (str) – The path to the audio file.\ndata (dict) – The data dictionary.\n\n\n\n\n\n\nabstract generate_from_text(text, cache_dir=None, path=None)[source]#\nImplement this method for each speech service. Refer to AzureService for an example.\n\nParameters:\n\ntext (str) – The text to synthesize speech from.\ncache_dir (str, optional) – The output directory to save the audio file and data to. Defaults to None.\npath (str, optional) – The path to save the audio file to. Defaults to None.\n\n\nReturns:\nOutput data dictionary. TODO: Define the format.\n\nReturn type:\ndict\n\n\n\n\n\nset_transcription(model=None, kwargs={})[source]#\nSet the transcription model and keyword arguments to be passed\nto the transcribe() function.\n\nParameters:\n\nmodel (str, optional) – The Whisper model to use for transcription. Defaults to None.\nkwargs (dict, optional) – Keyword arguments to pass to the transcribe() function. Defaults to {}.\n\n\n\n\n\n\n\nclass RecorderService(format=None, channels=1, rate=44100, chunk=512, device_index=None, transcription_model=\'base\', trim_silence_threshold=-40.0, trim_buffer_start=200, trim_buffer_end=200, callback_delay=0.05, **kwargs)[source]#\nBases: SpeechService\nSpeech service that records from a microphone during rendering.\nInitialize the speech service.\n\nParameters:\n\nformat (int, optional) – Format of the audio. Defaults to pyaudio.paInt16.\nchannels (int, optional) – Number of channels. Defaults to 1.\nrate (int, optional) – Sampling rate. Defaults to 44100.\nchunk (int, optional) – Chunk size. Defaults to 512.\ndevice_index (int, optional) – Device index, if you don’t want to choose it every time you render. Defaults to None.\ntranscription_model (str, optional) – The OpenAI Whisper model to use for transcription. Defaults to “base”.\n\ntrim_silence_threshold (float, optional) – Threshold for trimming silence in decibels. Defaults to -40.0 dB.\ntrim_buffer_start (int, optional) – Buffer duration for trimming silence at the start. Defaults to 200 ms.\ntrim_buffer_end (int, optional) – Buffer duration for trimming silence at the end. Defaults to 200 ms.\ncallback_delay (float) – \n\n\n\n\n\n\nclass AzureService(voice=\'en-US-AriaNeural\', style=None, output_format=\'Audio48Khz192KBitRateMonoMp3\', prosody=None, **kwargs)[source]#\nBases: SpeechService\nSpeech service for Azure TTS API.\n\nParameters:\n\nvoice (str, optional) – The voice to use. See the API page for all the available options. Defaults to en-US-AriaNeural.\nstyle (str, optional) – The style to use. See the API page to see how you can see available styles for a given voice. Defaults to None.\noutput_format (str, optional) – The output format to use. See the API page for all the available options. Defaults to Audio48Khz192KBitRateMonoMp3.\nprosody (dict, optional) – Global prosody settings to use. See the API page for all the available options. Defaults to None.\n\n\n\n\n\n\nclass CoquiService(model_name=\'tts_models/en/ljspeech/tacotron2-DDC\', config_path=None, vocoder_path=None, vocoder_config_path=None, progress_bar=True, gpu=False, speaker_idx=0, language_idx=0, **kwargs)[source]#\nBases: SpeechService\nSpeech service for Coqui TTS.\nDefault model: tts_models/en/ljspeech/tacotron2-DDC.\n\nParameters:\n\nglobal_speed (float, optional) – The speed at which to play the audio.\nDefaults to 1.00.\ncache_dir (str, optional) – The directory to save the audio\nfiles to. Defaults to voiceovers/.\ntranscription_model (str, optional) – The\nOpenAI Whisper model\nto use for transcription. Defaults to None.\n\ntranscription_kwargs (dict, optional) – Keyword arguments to\npass to the transcribe() function. Defaults to {}.\nmodel_name (str) – \nconfig_path (str) – \nvocoder_path (str) – \nvocoder_config_path (str) – \nprogress_bar (bool) – \n\n\n\n\n\ngenerate_from_text(text, cache_dir=None, path=None, **kwargs)[source]#\nImplement this method for each speech service. Refer to AzureService for an example.\n\nParameters:\n\ntext (str) – The text to synthesize speech from.\ncache_dir (str, optional) – The output directory to save the audio file and data to. Defaults to None.\npath (str, optional) – The path to save the audio file to. Defaults to None.\n\n\nReturns:\nOutput data dictionary. TODO: Define the format.\n\nReturn type:\ndict\n\n\n\n\n\n\nclass GTTSService(lang=\'en\', tld=\'com\', **kwargs)[source]#\nBases: SpeechService\nSpeechService class for Google Translate’s Text-to-Speech API.\nThis is a wrapper for the gTTS library.\nSee the gTTS documentation\nfor more information.\n\nParameters:\n\nlang (str, optional) – Language to use for the speech.\nSee Google Translate docs\nfor all the available options. Defaults to “en”.\ntld (str, optional) – Top level domain of the Google Translate URL. Defaults to “com”.\n\n\n\n\n\n\nclass OpenAIService(voice=\'alloy\', model=\'tts-1-hd\', transcription_model=\'base\', **kwargs)[source]#\nBases: SpeechService\nSpeech service class for OpenAI TTS Service. See the OpenAI API page\nfor more information about voices and models.\n\nParameters:\n\nvoice (str, optional) – The voice to use. See the\nAPI page\nfor all the available options. Defaults to "alloy".\nmodel (str, optional) – The TTS model to use.\nSee the API page\nfor all the available options. Defaults to "tts-1-hd".\n\n\n\n\n\n\nclass PyTTSX3Service(engine=None, **kwargs)[source]#\nBases: SpeechService\nSpeech service class for pyttsx3.\n\n\n\nDefaults#\n\n\nDEEPL_SOURCE_LANG = {\'bg\': \'Bulgarian\', \'cs\': \'Czech\', \'da\': \'Danish\', \'de\': \'German\', \'el\': \'Greek\', \'en\': \'English\', \'es\': \'Spanish\', \'et\': \'Estonian\', \'fi\': \'Finnish\', \'fr\': \'French\', \'hu\': \'Hungarian\', \'id\': \'Indonesian\', \'it\': \'Italian\', \'ja\': \'Japanese\', \'lt\': \'Lithuanian\', \'lv\': \'Latvian\', \'nl\': \'Dutch\', \'pl\': \'Polish\', \'pt\': \'Portuguese (all Portuguese varieties mixed)\', \'ro\': \'Romanian\', \'ru\': \'Russian\', \'sk\': \'Slovak\', \'sl\': \'Slovenian\', \'sv\': \'Swedish\', \'tr\': \'Turkish\', \'uk\': \'Ukrainian\', \'zh\': \'Chinese\'}#\nAvailable source languages for DeepL\n\n\n\nDEEPL_TARGET_LANG = {\'bg\': \'Bulgarian\', \'cs\': \'Czech\', \'da\': \'Danish\', \'de\': \'German\', \'el\': \'Greek\', \'en\': \'Alias for en-us\', \'en-gb\': \'English (British)\', \'en-us\': \'English (American)\', \'es\': \'Spanish\', \'et\': \'Estonian\', \'fi\': \'Finnish\', \'fr\': \'French\', \'hu\': \'Hungarian\', \'id\': \'Indonesian\', \'it\': \'Italian\', \'ja\': \'Japanese\', \'lt\': \'Lithuanian\', \'lv\': \'Latvian\', \'nl\': \'Dutch\', \'pl\': \'Polish\', \'pt\': \'Alias for pt-pt\', \'pt-br\': \'Portuguese (Brazilian)\', \'pt-pt\': \'Portuguese (all Portuguese varieties excluding Brazilian Portuguese)\', \'ro\': \'Romanian\', \'ru\': \'Russian\', \'sk\': \'Slovak\', \'sl\': \'Slovenian\', \'sv\': \'Swedish\', \'tr\': \'Turkish\', \'uk\': \'Ukrainian\', \'zh\': \'Chinese (simplified)\'}#\nAvailable target languages for DeepL\n\n\n\n\n\n\n\n\n\n\n\nPrevious\n\nTranslating Scenes\n\n\n\n\n\n\n                Copyright © 2022, The Manim Community Dev Team\n            \n            Made with Sphinx and @pradyunsg\'s\n            \n            Furo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            On this page\n          \n\n\n\n\nAPI Reference\nVoiceover scene\nVoiceoverScene\nVoiceoverScene.add_voiceover_text()\nVoiceoverScene.add_wrapped_subcaption()\nVoiceoverScene.safe_wait()\nVoiceoverScene.set_speech_service()\nVoiceoverScene.voiceover()\nVoiceoverScene.wait_for_voiceover()\nVoiceoverScene.wait_until_bookmark()\n\n\nVoiceoverTracker\nVoiceoverTracker.get_remaining_duration()\nVoiceoverTracker.time_until_bookmark()\n\n\n\n\nSpeech services\nSpeechService\nSpeechService.audio_callback()\nSpeechService.generate_from_text()\nSpeechService.set_transcription()\n\n\nRecorderService\nAzureService\nCoquiService\nCoquiService.generate_from_text()\n\n\nGTTSService\nOpenAIService\nPyTTSX3Service\n\n\nDefaults\nDEEPL_SOURCE_LANG\nDEEPL_TARGET_LANG\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'